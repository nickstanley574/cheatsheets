# SE441: Continuous Delivery and DevOps Lecture Notes
**Winter 2018 - 2019**
**Prof. Christopher Jones**


## Introduction

A IDC Analyst showed that on average **80%** of system outages are caused by operator or application errors. Everything else days empathizes speed: agile development, business agility etc. Speed might be a goal but it's an outcome! You don't get fast by just going fast. Greyhournd know how to run but it takes training to get them to RUN month and month of ward works. 

### The Myth of Technical Excellence
The Myth of Technical Excellence - whenever we have really hard problems to solve we tend to look for tech-based solutions. THIS IS VERY WRONG! Good Tech is necessary but not sufficient, Software is a **human endeavor** and its useless if there aren't good process to support them, which means CD is about Tech and PEOPLE. We don't solve tech problems we solve people problems. 

Constant incremental improvements typically lead to greater gains than leader, periodic leaps. We are looking at evolving our processes, practices, and people rather than revolutionizing them. 

Software development is essentially a series of experiments. What we delivery (VALUE) and how we deliver it (PROCESS). 

New Technology and tools DO NOT fix a culture, person, or process problem. Comparatively speaking, computers are easy, people are harder. The human factors trumps everything, without voluntary adoption, people find ways to subvert tech. In fairness, there are a lot of problems that can be solved with good tech tooling, but the adoption of too many techs can lead to integration and process problems. 

If CD is the goal we need to emphasizes: 
1. Building Quality in. (We can't compromise quality for speed)
2. Working in small batches
3. Let the computer do the repetitive work and let people solve problems. (Thats more fun anyway)
4. Relentlessly pursue continuous improvement. 
5. Everyone is responsible. 

In time of you focus on these 5 items you will BECOME fast. 

Too often the content of software is thought to be code, the content is business value code is just the mean of delivering that value. Reliably identification and prioritization of values is CRITICAL **(IF YOU CAN'T DO THIS YOU WILL FAIL)**. If you're building the wrong things, fast deploys only ensure that you ship the wrong things faster. (Not a great ROI story) 

Big bangs changes can cause distractions. 
1. Development or testing churn. 
2. Downtime (planned or unplanned) 
3. Let the computer do the repetitive work and let people SOLVE PROBLEMS 
4. Relentlessly purse continuous improvement 
5. Everyone is responsible 

Small batches allowed delivering constant flow of business value.

Doing CD require organizational change! 

Need to balance tech with people; neither one alone guarantee success. 

### Software Economics

All for-profit businesses have the same basic goal – to make money. 

Simple ways to measure ROI:
- Saving by reducing the amount  of unplanned work 
- Saving by reducing outages from bad releases
- Values of activities enabled by the first two options. 

```
cost_of_excess_rework = tech_staff_size * 
                        average_salary * 
                        benefits_multiplier *
                        percent_of_time_on_rework

cost_of_downtime      = deploy_frequency *
                        change_failure_rate (%) *
                        mean_time_to_recover *
                        hourly_outage_cost
```

### Continuous Delivery and DevOps 

Development is also the “experts” when it comes to how its application functions and what it requires to operate successfully. Operations is responsible for the runtime operation and environments of the delivered software.

DevOps promotes a set of processes and methods for thinking about communication and collaboration between development, QA, and IT operations.

According to Martin Fowler, you're doing CD when: 
1. Software is deployable at any time 
2. Deployability is more important than new features 
3. Fast, automated feedback on the prod readiness of all system on any change.
4. On-demand push-button deployments, any version, and environment. 

The Goal is to have low cycle-time (plan -> code -> build -> test -> release -> deploy -> operate -> monitor -> repeat), and high quality releases. 

DevOps Principles: 
1. Every Change leads to a potential release. 
2. Must have repeatable, reliable process for software releases.
3. Automate almost everything. 
4. Keep everything in version control.
5. If it hurts, do it often and bring the pain forward. 
6. Build quality in. 
7. Done means released.
8. Everyone is responsible for the delivery process. 
9. Continues improvement 

### Risks and Rewards 

Release should be Repeatable, Reliable, and Predictable. 

No developing in prod-like environments can lead to: (1) Inconsistencies in hardware configs (2)  Inconsistent behavior between lower, test envois and higher, prod envs. (3)  An inability to test hardware and software configs before they are introduced into higher-envs.   

**No one cares if it works on your machine.** Higher environments may be the first time the operations team has
seen the release. All installers, assemblies, configurations, etc. have only been tested in environments that are nothing like production. Different cluster members have different versions of OS, patches, middleware, libraries, etc. **We need to treat configuration like code.**

**One technique: frequent releases!**
- Rewards: 
    - The more frequent the release, the less change is needed.
    - Rollbacks are less risky.	
    - Feedback is faster.
    - Value is realized more quickly.
- Risk: Cost of bugs getting into production.
    - Assumptions: 
        - We know what our customers want. 
        - Bugs are expensive.
        - Testing takes a long time. 
        - All bugs can be found in test.
- Risk: New releases will annoy users.
    - Assumptions: 
        - Releases incur downtime.
	    - Users will notice changes.
	    - Changes are visible to all users.
- Risk: Cost of release is greater than its value.
    - Assumptions: 
        - High coordination overhead. 
        - Releases are risky.
        - Deployment is time consuming.

**Contrary to conventional wisdom, we need to embrace failure – it’s not only always an option, it’s inevitable. WE need to Learn how to fail better! Use cheaper, less reliable hardware. If processes are risky, do them often. Don’t punish people for outages.**

## Theory of Constraints and Value Steam

### Value Stream Mapping 

**Value-stream mapping** is a lean-management method for analyzing the current state and designing a future state for the series of events that take a product or service from its beginning through to the customer with reduced lean wastes as compared to current map.

Value Stream Mapping offers:
- Offers a holistic view of how work moves through a system.
- Maps of material and information flows across the organization.
- Captures value-added work from non-value-added work.

Useful Metrics: 
- **Total lead time (TLT)** - The time needed to complete a process from the time work is ready until that work is complete. 
- **Total Process time (TPT)** - The total time required to actually complete a process. This is typically much less than the TLT. 
- **Process cycle efficiency (PCE)** - `Total process time / total lead time`. The Percentage of the TLT that involves actual work. 
- **Percent complete and accurate (%C&A)** - The percentage of work that each of a processes' downstream consumers considers ready. Anything less Then 100% represents waste.  
- **Rolled  %C&A** -  The overall %C&A for the entire value stream. This is just the %C&A of each process in the value stream multiplied together. 
- **Takt Time** - It's the cycle time ended to keep up with customer demand. Takt time = `available production time per-day / Demand per day`. (IF the Toal lead time is greater then take time the org will not keep up with demand.)

### The Theory of Constraints

**The theory of constraints (TOC)** is a management paradigm that views any manageable system as being limited in achieving more of its goals by a very small number of constraints. There is always at least one constraint, and TOC uses a focusing process to identify the constraint and restructure the rest of the organization around it. TOC adopts the common idiom "a chain is no stronger than its weakest link". This means that processes, organizations, etc., are vulnerable because the weakest person or part can always damage or break them or at least adversely affect the outcome. (**Every value stream or workflow has some constraint in it  and unless you deal with it in some faction any other place you optimize doesn’t matter.**)




Simple/basic premises: 
1. Every system is constrained in some way
2. Any changes to the system that do not improve throughput at the constraint cannot improve the throughput of the overall system.

## The Three Ways

### 1. The Principles of FLOW: 

- Make work visible
    - Use visual management.    
    - Avoid trigger work from emails and chats - all work should flow through the same process of prioritization.
    - Make sure that each work center has its own queue of work.
    - Use these work boards to capture important metrics
- Limit Work in Process (WIP)
    - People are horrible and multitasking (THIS IS FACT) 
    - Limit the number of concurrent tasks
    - Wait times need to be managed. `wait_time = % Resource is Busy / % Resource is idle`
- Reduce batch sizes
    - We want to deliver value as rapidly as possible.
- Reduce the number of handoffs
    - Information is always lost during handoffs.
    - Handoffs require time and effort to setup and coordinate.
- Continually identify and elevate constraints
    - To optimize:
        1. Identify the limiting constraint(s) in the stream.
        2. Exploit the constraint. If the constraint is idle, that value is lost forever.
        3. Subordinate everything to the constraint. This means other resources may be underutilized.
        4. Delays upstream don’t usually average out farther downstream other than by accident.
        5. Elevate the constraint. If cycle time is too long, there is more work to do.
        6. Find the next constraint and repeat the process.
    - Eliminate hardship and waste in the value stream
	    - Extra processes that add no value to the customer.
	    - Extra features that don’t meet either business or customer needs.
	    - Task switching.
	    - Heroics (DON'T BE A HERO YOU ARE NOT SUPERMAN YOU WILL HATE YOU LIFE ... Process > Heros) 

### 2. The Principles of FEEDBACK

Problems need to be visible. Feedback and feed forward loops can help address this. 

Prevents problems from moving downstream – easier and cheaper to fix them close to the point of origination.

Adding more steps to a process often makes that process even less reliable because it can lead to significant deviations between who should do a certain task who actually does that task. (Example Most documentation.) 

### 3. The Principles of CONTINUAL LEARNING AND EXPERIMENTATION. 

Common organizational cultures:
- **Pathological** - Large amounts of fear and threat. Information is hoarded or used for political gain. Failure is hidden. (RUN AWAY) 
- **Bureaucratic** - Governed by rules and processes. Teams defend their own area of responsibility. Failure works through some form of judgement. (Depends on the business demands can be good often is not needed) 
- **Generative** - Actively seeks and shares information. Responsibilities are shared across the value stream. Failure is reflected and improved upon. (This is the goal) 

If processes are not actively improved, they’ll degrade over time. 

Do not shy away from disruptions or failure. Become resilient by learning how to react too and recover from inevitable problems. Make opportunities to experiment and try new things so that you know where the failure points are in your organization and systems and how to deal with them.

Leaders encourage their people to embrace learning. Encourage employee experimentation by asking questions.

## Source Code Management and GIT

**Configuration Management** refer to the process by which all artifacts relevant to your project, and the relationships between them, are stored, retrieved uniquely identified and modified. 

Source code management tools, often referred to as “version control”, are the best way to manage source code and related assets. Concurrent, consistency, history and recoverability. 

Good configuration management provides benefits: Any environment including its OS, patch level, network configuration, software stack, applications, and application configuration can be reproduced on demand. Incremental changes to any of the above can be easily made and deployed across environments. Changes to each environment can be traced to find out when the
change was made, who made it, and why. **All compliance regulations and requirements are satisfied.**

- **Concurrent Modification Model:** Work is easily lost. He who saves last wins.
- **Lock-Modify-Unlock:** No concurrent development on shared files. Developers frequently forget to unlock their files.
- **Copy-Modify-Merge:** Allows concurrency. Merges can be painful.

**Version control is the single most important tool in the DevOps and continuous delivery space.**

### Git Basics

Earlier SCMs managed changes by tracking the deltas from one version to the next. Git stores the entire file rather than the deltas. Requires more disk, but disk is NOW MUCH cheap compared to developer time. Linus Torvalds, father of Linux, wanted an SCM that could keep up with the Linux contributor community.

A repository holds everything under Git’s control for a particular project. It contains a complete history of all of the resources and activities that have ever been part of the project.
- **Blobs (Binary large objects)** store the actual data about each file within the repository. Every object in the repository is given a SHA1 hash. This hash allows git to detect changes to the blob.
- **Trees** store the metadata about blobs including: Identifiers Pathnames and Links to other trees.
- **Commits** store the metadata about shareable changes including: Committer, Commit date, Log message, A link to a tree representing the state of the repository. 
- **Tags** store the metadata including: Tag name, Commit identifier.

### Branching Models and Feature Flags 

**Branching models significantly impact release rate.**

Centralized workflow:
- Branches are created for each change or feature
- Branches are pushed to the central server and a pull request is created.
- The change is reviewed before being merged into the Master branch.

“Gitflow” workflow:
- Master represents production-ready code. Branched to Develop and Hotfix.
- Hotfix represents emergency changes. Merged to Master and Develop.
- Develop represents latest delivered changes for next release. Branched to Feature branches. Merged to Release.
- Release represents a staging area for the next release.
- Feature branches contain individual changes or features. Always merged back to Develop.

Merges are evil. In the best case we could avoid them. One such approach is called **trunk-based** or **mainline development**. 
- Each merge is cherry-picked – we don’t try to bring every change into the release; only the most important ones.
- One branch is created for each release.
- Developers can only commit to the trunk, not to the release branch.
- Release engineers create the branches and cherry-pick individual commits from the trunk into them.
- Developers should not break the build with any commit.
- Merges back to trunk are few and far between – only if an issue cannot be reproduced in the trunk itself.
- If all commits are done against trunk, how do we handle new features? We need a way to disable features until they’re ready to be released. he answer is a technique called **feature flags** or **feature toggles**. 

### Software Configuration

- Too much time spent in designing configurations (which need to be tested) rather than solving the actual problem.
- Configurations usually can’t be tested until run-time.

Configuration can be injected at: Build time, Packaging time, Deployment time, Run time

We often need an application’s configuration to vary at different levels of granularity: Environment, Data Center, Tier, Server. **This can lead to configuration file “bloat” where many different configuration files need to be managed.** 

## Build Automation 

Automation Benefits: 
- **Repeatability**: The build will always follow the same sequence of steps and generate the same outputs. 
- **Consistency**: The steps executed during the build will be the same regardless of the person executing the build. 
- **Portability**: The build process is tried to the product and not to any particular developer's machines or environment.
- **Integration**: The build can be executed as part of an automated CI process.    

Java Automation tools: 

| Tool  | Code      | Style                     | Lifecycle |
| ---   | ---       | ---                       | ---       |   
| Ant   | XML       | Imperative                | None      |
| Maven | XML       | Declarative               | Depends on artifact type and applied plugins |
| Gradle| Groovy DSL| Imperative & Declarative  | Depends on artifact type and applied plugins |

- Ant provides no default lifecycle save the guarantee that it will attempt ,to execute whatever target you specify on the command line or the default target otherwise.
- Maven’s default lifecycle provides for things like compilation, testing, and packaging. The lifecycle will change based on the kind of artifact being built.
- Gradle’s lifecycle is more complex:
    - **Initialization**: Gradle determines what projects will participate in the build.
    - **Configuration**: Gradle determines what takes will be performed and defines the DAG (directed acyclic graph) by which they will be executed. 
    - **Execution**: The tasks in the DAG are actually performed.

### Dependency and Artifact Management

Software artifacts generally do not work in isolation. They usually depend on other software artifacts. The process of maintaining the various artifacts that support our applications is called **dependency management**.

Dependency management can be one of two types:
- **manual**: A developer is responsible for downloading all of the required dependencies as well as their dependencies. (Can require a significant time investment. Increased risk of error.)
- **automated**: In an automated approach, the dependencies of an artifact are declared. (Time investment can be less. Ris of error is reduced.) 

Best Practices:
- Version numbers in the file name.
- Manage transitive dependencies
- Resolve version conflicts

The `groupId`, `artifactId`, and `version` comprise the artifact’s coordinates. 

**Ant + Ivy (build.xml)**

Ant by itself doesn’t provide any form of dependency management. For that we need to use Ivy. Ivy can use all Maven repositories.

- `ivy:resolve`. Calculate the dependencies and download them to the local Ivy cache.
- `ivy:install`. Installs modules from an public repository into an enterprise repository.
- `ivy:retrieve`. Retrieves all artifacts into a local directory.
- `ivy:publish`. Publishes new artifacts to the enterprise repository.

**Maven (pom.xml)**

Maven recognizes six (6) different scopes, but we only care about the following four (4):
- `compile`. The dependency appears on all classpaths.
- `test`. The dependency appears on the test classpath.
- `runtime`. The dependency appears on the runtime classpath.
- `provided`. The dependency appears on the compile and test classpaths.

**Gradle (build.gradle)** 

Gradle uses the same dependency resolution process as Maven by default.

In a nutshell, the process is as follows:
1. Look for the artifact in the local cache. If it’s there, add that path to the classpath.
2. If it isn’t in the local cache, start looking at each remote repository that the Maven project knows about.
3. Look for the artifact in the remote repository. If it’s there, download it into the local cache and add that path to the classpath.
4. If it wasn’t found in the remote repository, proceed to the next remote repository and check again. Keep checking until we either run out of repositories or until we successfully find and download the artifact.

The fact that we can download artifacts from the internet provides both opportunities and challenges:
**Pros:** Convenient, Inherent redundancy, Standard practice.  
**Cons:** Variability in controls, Relies on goodwill and No way to prevent downloads
We can use an artifact repository to help mitigate the risks of using automate dependency management.

An **artifact repository** is software that intermediates between your build scripts and the internet. They typically provide Repository proxying, Artifact caching, Artifact blacklisting and Security and auditing. It becomes a time machine that contains a complete history of your deployable artifacts.

